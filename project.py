# -*- coding: utf-8 -*-
"""Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JqiSfwP-rqgj3cEdgDA60ipjgEgv-2Oa
"""

# Importing necessary libraries, classes, and functions
import numpy as np
from sklearn.model_selection import train_test_split
import pandas as pd
import seaborn
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay
from sklearn.decomposition import PCA
from sklearn import svm
from sklearn.manifold import TSNE
from sklearn.neighbors import KNeighborsClassifier as KNN
import tensorflow as tf
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn import tree

# Import and format data
data = np.loadtxt('agaricus-lepiota.data',usecols = (23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45))
target = data[:,0]
data = data[:,1:]
feature_names = ['cap shape','cap surface','cap color','bruises?','odor','gill attachment','gill spacing','gill size','gill color','stalk shape','stalk root','stalk surface above ring','stalk surface below ring','stalk color above ring','stalk color below ring','veil type','veil color','ring number','ring type','spore print color','population','habitat']
class_names = ['Edible','Poisonous']
X_train,X_test,y_train,y_test = train_test_split(data,target,test_size = 0.3,stratify = target) # train-test split data
pdf = pd.DataFrame(data) # put data in pandas dataframe

# Create feature pairplot for data visualization
seaborn.pairplot(pdf)

# Random Forest Mean Decrease in Impurity (MDI) Feature Importance
forest = RandomForestClassifier()
forest.fit(X_train,y_train)
importances = forest.feature_importances_
std = np.std([tree.feature_importances_ for tree in forest.estimators_],axis=0)
forest_importances = pd.Series(importances)
fig, ax = plt.subplots()
forest_importances.plot.bar(yerr=std, ax=ax)
ax.set_title("Feature Importances using MDI")
ax.set_ylabel("Mean Decrease in Impurity")
fig.tight_layout()

# Principal Component Analysis (PCA) - (95% variance explained)
plt.figure()
pca_95 = PCA(n_components = 0.95) # 95% of variance described by principal features desired
pca_95.fit(data)
reduced_95 = pca_95.transform(data)

# PCA plotting (95% variance) (modified code from https://www.mikulskibartosz.name/pca-how-to-choose-the-number-of-components/)
plt.rcParams["figure.figsize"] = (12,6)
fig, ax = plt.subplots()
xi = np.arange(1, 14, step=1)
y = np.cumsum(pca_95.explained_variance_ratio_)
plt.ylim(0.0,1.1)
plt.plot(xi, y, marker='o', linestyle='--', color='b')
plt.xlabel('Number of Components')
plt.xticks(np.arange(0, 14, step=1)) #change from 0-based array index to 1-based human-readable label
plt.ylabel('Cumulative Variance (%)')
plt.title('Number of Components Needed to Explain 95% of Variance')
plt.axhline(y=0.95, color='r', linestyle='-')
plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)
ax.grid(axis='x')
plt.show()
print(pca_95.explained_variance_ratio_)

# PCA (99% variance explained)
plt.figure()
pca_99 = PCA(n_components = 0.99) # 99% of variance described by principal features desired
pca_99.fit(data)
reduced_99 = pca_99.transform(data)

# PCA plotting (99% variance) (modified code from https://www.mikulskibartosz.name/pca-how-to-choose-the-number-of-components/)
plt.rcParams["figure.figsize"] = (12,6)
fig, ax = plt.subplots()
xi = np.arange(1, 18, step=1)
y = np.cumsum(pca_99.explained_variance_ratio_)
plt.ylim(0.0,1.1)
plt.plot(xi, y, marker='o', linestyle='--', color='b')
plt.xlabel('Number of Components')
plt.xticks(np.arange(0, 18, step=1)) #change from 0-based array index to 1-based human-readable label
plt.ylabel('Cumulative Variance (%)')
plt.title('Number of Components Needed to Explain 99% of Variance')
plt.axhline(y=0.99, color='r', linestyle='-')
plt.text(0.5, 0.85, '99% cut-off threshold', color = 'red', fontsize=16)
ax.grid(axis='x')
plt.show()
print(pca_99.explained_variance_ratio_)

# Support Vector Classifier (SVC) - (95% reduced PCA data) implementation and results
X_train_95,X_test_95,y_train_95,y_test_95 = train_test_split(reduced_95,target, test_size = 0.4)
clf_95 = svm.SVC()
clf_95.fit(X_train_95,y_train_95)
y_pred_95 = clf_95.predict(X_test_95)

print("Accuracy (svm95):",accuracy_score(y_test_95, y_pred_95))
print("Recall (svm95):",recall_score(y_test_95, y_pred_95, average = 'macro'))
print("Precision (svm95):",precision_score(y_test_95, y_pred_95, average = 'macro'))
cm95_svc = confusion_matrix(y_test_95, y_pred_95)
ConfusionMatrixDisplay(cm95_svc).plot(cmap = 'Reds')

# SVC (99% reduced PCA data) implementation and results
X_train_99,X_test_99,y_train_99,y_test_99 = train_test_split(reduced_99,target, test_size = 0.4)
clf_99 = svm.SVC()
clf_99.fit(X_train_99,y_train_99)
y_pred_99 = clf_99.predict(X_test_99)

print("Accuracy (svm99):",accuracy_score(y_test_99, y_pred_99))
print("Recall (svm99):",recall_score(y_test_99, y_pred_99, average = 'macro'))
print("Precision (svm99):",precision_score(y_test_99, y_pred_99, average = 'macro'))
cm99_svc = confusion_matrix(y_test_99, y_pred_99)
ConfusionMatrixDisplay(cm99_svc).plot(cmap = 'Reds')

# K-Nearest Neighbors - KNN (95% reduced PCA data) implementation and results
KNN_95 = KNN(n_neighbors = 5, metric = 'minkowski',p = 2)
KNN_95.fit(X_train_95,y_train_95)
y_pred_95_knn = clf_95.predict(X_test_95)
print("Accuracy (knn95):",accuracy_score(y_test_95, y_pred_95_knn))
print("Recall (knn95):",recall_score(y_test_95, y_pred_95_knn, average = 'macro'))
print("Precision (knn95):",precision_score(y_test_95, y_pred_95, average = 'macro'))
cm95_knn = confusion_matrix(y_test_95, y_pred_95_knn)
ConfusionMatrixDisplay(cm95_knn).plot(cmap = 'Reds')

# K-Nearest Neighbors - KNN (99% reduced PCA data) implementation and results
KNN_99 = KNN(n_neighbors = 5, metric = 'minkowski',p = 2)
KNN_99.fit(X_train_99,y_train_99)
y_pred_99_knn = clf_99.predict(X_test_99)
print("Accuracy (knn99):",accuracy_score(y_test_99, y_pred_99_knn))
print("Recall (knn99):",recall_score(y_test_99, y_pred_99_knn, average = 'macro'))
print("Precision (knn99):",precision_score(y_test_99, y_pred_99, average = 'macro'))
cm99_knn = confusion_matrix(y_test_99, y_pred_99_knn)
ConfusionMatrixDisplay(cm99_knn).plot(cmap = 'Reds')

# Gaussian Naive Bayes implementation and results
gnb = GaussianNB().fit(X_train,y_train)
print(gnb.score(X_test,y_test))
y_pred_gnb = gnb.predict(X_test)
print("Accuracy (gnb):",accuracy_score(y_test, y_pred_gnb))
print("Recall (gnb):",recall_score(y_test, y_pred_gnb, average = 'macro'))
print("Precision (gnb):",precision_score(y_test, y_pred_gnb, average = 'macro'))
cm_gnb = confusion_matrix(y_test, y_pred_gnb)
ConfusionMatrixDisplay(cm_gnb).plot(cmap = 'Reds')

# Decision tree classifier implementation and results
dtc = DecisionTreeClassifier().fit(X_train,y_train)
y_pred_dtc = dtc.predict(X_test)
print("Accuracy (dtc):",accuracy_score(y_test, y_pred_dtc))
print("Recall (dtc):",recall_score(y_test, y_pred_dtc, average = 'macro'))
print("Precision (dtc):",precision_score(y_test, y_pred_dtc, average = 'macro'))
cm_dtc = confusion_matrix(y_test, y_pred_dtc)
ConfusionMatrixDisplay(cm_dtc).plot(cmap = 'Reds')
plt.figure(figsize = (20,10))
tree.plot_tree(dtc,feature_names = feature_names, class_names = class_names, filled = True)

# Logistic Regression implementation and results
lr = LogisticRegression().fit(X_train,y_train)
y_pred_lr = lr.predict(X_test)
print("Accuracy (lr):",accuracy_score(y_test, y_pred_lr))
print("Recall (lr):",recall_score(y_test, y_pred_lr, average = 'macro'))
print("Precision (lr):",precision_score(y_test, y_pred_lr, average = 'macro'))
cm_lr = confusion_matrix(y_test, y_pred_lr)
ConfusionMatrixDisplay(cm_lr).plot(cmap = 'Reds')

